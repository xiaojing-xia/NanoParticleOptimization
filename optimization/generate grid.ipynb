{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ca3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f9592ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_radius = np.arange(5,34,2)\n",
    "conc_interval = 0.01\n",
    "range_sum1 = np.linspace(0,1,int(1/conc_interval +1))\n",
    "range_sum2 = np.linspace(0,1,int(1/conc_interval +1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d4cedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(columns=['yb_1','er_1', 'yb_2','er_2', 'radius'])\n",
    "def generate_grid(range_radius,range_sum1, range_sum2):\n",
    "    pool = []\n",
    "    for r in range_radius:\n",
    "        print(f'radius = {r}')\n",
    "        for i in range_sum1:\n",
    "            #print(f'sum1 = {i}')\n",
    "            for j in range_sum2:\n",
    "                sum1 = i\n",
    "                yb_1_num = int(sum1/conc_interval + 1)\n",
    "                sum2 = j\n",
    "                yb_2_num = int(sum2/conc_interval + 1)\n",
    "                range_yb1 = np.linspace(0,sum1, yb_1_num)\n",
    "                range_yb2 = np.linspace(0,sum2, yb_2_num)\n",
    "                for yb1 in range_yb1:\n",
    "                    for yb2 in range_yb2:\n",
    "                        #df.loc[len(df.index)] = [yb1, sum1-yb1, yb2, sum2-yb2, r]\n",
    "                        pool.append([yb1, sum1-yb1, yb2, sum2-yb2, r])\n",
    "    return pool\n",
    "#df = pd.DataFrame(columns=['yb_1','er_1', 'yb_2','er_2', 'radius'])\n",
    "def generate_grid_encoded(range_radius,range_sum1, range_sum2):\n",
    "    pool = []\n",
    "    for r in range_radius:\n",
    "        print(f'radius = {r}')\n",
    "        for i in range_sum1:\n",
    "            #print(f'sum1 = {i}')\n",
    "            for j in range_sum2:\n",
    "                sum1 = i\n",
    "                yb_1_num = int(sum1/conc_interval + 1)\n",
    "                sum2 = j\n",
    "                yb_2_num = int(sum2/conc_interval + 1)\n",
    "                range_yb1 = np.linspace(0,sum1, yb_1_num)\n",
    "                range_yb2 = np.linspace(0,sum2, yb_2_num)\n",
    "                for yb1 in range_yb1:\n",
    "                    for yb2 in range_yb2:\n",
    "                        #df.loc[len(df.index)] = [yb1, sum1-yb1, yb2, sum2-yb2, r]\n",
    "                        if sum1 == 0:\n",
    "                            ratio1 = 0.5\n",
    "                        else:\n",
    "                            ratio1 = yb1/sum1\n",
    "                        if sum2 == 0:\n",
    "                            ratio2 = 0.5\n",
    "                        else:\n",
    "                            ratio2 = yb2/sum2\n",
    "                        pool.append([sum1, ratio1, sum2, ratio2, r/34])\n",
    "    return pool\n",
    "\n",
    "def generate_grid_encoded_radius(radius,range_sum1, range_sum2):\n",
    "    pool = []\n",
    "    \n",
    "    for i in range_sum1:\n",
    "        #print(f'sum1 = {i}')\n",
    "        for j in range_sum2:\n",
    "            sum1 = i\n",
    "            yb_1_num = int(sum1/conc_interval + 1)\n",
    "            sum2 = j\n",
    "            yb_2_num = int(sum2/conc_interval + 1)\n",
    "            range_yb1 = np.linspace(0,sum1, yb_1_num)\n",
    "            range_yb2 = np.linspace(0,sum2, yb_2_num)\n",
    "            for yb1 in range_yb1:\n",
    "                for yb2 in range_yb2:\n",
    "                    #df.loc[len(df.index)] = [yb1, sum1-yb1, yb2, sum2-yb2, r]\n",
    "                    if sum1 == 0:\n",
    "                        ratio1 = 0.5\n",
    "                    else:\n",
    "                        ratio1 = yb1/sum1\n",
    "                    if sum2 == 0:\n",
    "                        ratio2 = 0.5\n",
    "                    else:\n",
    "                        ratio2 = yb2/sum2\n",
    "                    pool.append([sum1, ratio1, sum2, ratio2, (radius/34)])\n",
    "    return pool       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9bfc351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius = 5\n",
      "radius = 7\n",
      "radius = 9\n",
      "radius = 11\n",
      "radius = 13\n",
      "radius = 15\n",
      "radius = 17\n",
      "radius = 19\n",
      "radius = 21\n",
      "radius = 23\n",
      "radius = 25\n",
      "radius = 27\n",
      "radius = 29\n",
      "radius = 31\n",
      "radius = 33\n"
     ]
    }
   ],
   "source": [
    "pool_encoded = generate_grid_encoded(range_radius,range_sum1, range_sum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_encoded = torch.FloatTensor(pool_encoded)\n",
    "pickle.dump(pool_encoded, open( \"NP_pool_medium_conc1_radi2_397528560NP_encoded.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "094f070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating radius =5\n",
      "Took 607.4734437465668 s\n",
      "Tensorizing\n",
      "Took 7.964658260345459 s\n",
      "Dumping to pickle\n",
      "Took 1.0307667255401611 s\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename='NP_pool_medium_conc1_radi2_397528560NP_encoded.pkl'\n",
    "for r in range_radius[:1]:\n",
    "    t0=time.time()\n",
    "    print(f'Generating radius ={r}')\n",
    "    pool_r = generate_grid_encoded_radius(r,range_sum1, range_sum2)\n",
    "    t1 = time.time()\n",
    "    print(f'Took {t1-t0} s')\n",
    "    print(f'Tensorizing')\n",
    "    pool_rr = torch.FloatTensor(pool_r)\n",
    "    t2 = time.time()\n",
    "    print(f'Took {t2-t1} s')\n",
    "    print(f'Dumping to pickle')\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(pool_rr,fp)\n",
    "    t3 = time.time()\n",
    "    print(f'Took {t3-t2} s')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e91afd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : omp.h header is not in the path, disabling OpenMP.\n",
      "[KeOps] Warning : Cuda libraries were not detected on the system ; using cpu only mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<stdin>:1:10: fatal error: 'omp.h' file not found\n",
      "#include <omp.h>\n",
      "         ^~~~~~~\n",
      "1 error generated.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "\n",
    "from fireworks import LaunchPad\n",
    "from jobflow import JobStore\n",
    "from jobflow.managers.fireworks import flow_to_workflow\n",
    "from maggma.stores.mongolike import MongoStore\n",
    "from NanoParticleTools.flows.flows import get_npmc_flow\n",
    "from NanoParticleTools.inputs.nanoparticle import SphericalConstraint\n",
    "import uuid\n",
    "\n",
    "from botorch.models import SingleTaskGP, ModelListGP\n",
    "from botorch import fit_gpytorch_model\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "from botorch.acquisition.monte_carlo import qUpperConfidenceBound\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "from common import seed_generator, configs\n",
    "\n",
    "from common.utils import get_int, get_qe\n",
    "from common import utils\n",
    "\n",
    "def get_data_botorch(data_file, from_cloud = True):\n",
    "    if from_cloud:\n",
    "        df = gcloud_utils.get_df_gspread(GSPREAD_CRED, GSPREAD_NAME)\n",
    "        #df = df.drop(labels=range(1, 600), axis=0)\n",
    "        my_data = df.to_numpy()\n",
    "        print(f\"reading data log from google sheet: {GSPREAD_NAME}!\")\n",
    "    else:\n",
    "        my_data = np.loadtxt(data_file, delimiter=',', skiprows=1)\n",
    "        print(f\"reading data log from local: {data_file}!\")\n",
    "\n",
    "    # features\n",
    "    train_x = torch.from_numpy(my_data[:, :5])\n",
    "    # labels\n",
    "    train_y = torch.from_numpy(my_data[:, 5]).unsqueeze(-1)\n",
    "    # best observation\n",
    "    best_y = train_y.max().item()\n",
    "    \n",
    "    return train_x, train_y, best_y\n",
    "\n",
    "\n",
    "def encode_inputs(x_arr, x_max = 34):\n",
    "    '''encode simulation input to botorch'''\n",
    "    for i, arr in enumerate(x_arr):\n",
    "        x_arr[i, 0] = arr[0] + arr[1]\n",
    "        if arr[0] + arr[1] == 0:\n",
    "            x_arr[i, 1] = 0.5\n",
    "        else:\n",
    "            x_arr[i, 1] = arr[0] / (arr[0] + arr[1])\n",
    "        x_arr[i, 2] = arr[2] + arr[3]\n",
    "        if arr[2] + arr[3] == 0:\n",
    "            x_arr[i, 3] = 0.5\n",
    "        else:\n",
    "            x_arr[i, 3] = arr[2] / (arr[2] + arr[3])\n",
    "        x_arr[i, 4] = arr[4] / x_max\n",
    "\n",
    "\n",
    "def decode_candidates(x_arr, x_max = 34):\n",
    "    '''decode botorch recommendation candidates for simulation'''\n",
    "    for i, arr in enumerate(x_arr):\n",
    "        x_arr[i, 0], x_arr[i, 1] = arr[0] * arr[1], arr[0] * (1 - arr[1])\n",
    "        x_arr[i, 2], x_arr[i, 3] = arr[2] * arr[3], arr[2] * (1 - arr[3])\n",
    "        x_arr[i, 4] = arr[4] * x_max\n",
    "\n",
    "def recommend(train_x, train_y, best_y, bounds, n_trails = 5):\n",
    "    if isinstance(bounds, list):\n",
    "        bounds = torch.tensor(bounds)\n",
    "    elif torch.is_tensor(bounds):\n",
    "        pass\n",
    "    else:\n",
    "        raise TypeError(f\"expect bounds in a list or tensor. was given {type(bounds)}\")\n",
    "    \n",
    "    single_model = SingleTaskGP(train_x, train_y)\n",
    "    mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    \n",
    "    # Expected Improvement acquisition function\n",
    "    EI = qExpectedImprovement(model = single_model, best_f = best_y)\n",
    "    # Upper Confidence Bound acquisition function\n",
    "    UCB = UpperConfidenceBound(single_model, beta=100)\n",
    "    \n",
    "    # hyperparameters are super sensitive here\n",
    "    candidates, _ = optimize_acqf(acq_function = UCB,\n",
    "                                 bounds = bounds, \n",
    "                                 q = n_trails, \n",
    "                                 num_restarts = 20, \n",
    "                                 raw_samples = 512, \n",
    "                                # options = {'batch_limit': 5, \"maxiter\": 200}\n",
    "                                 )\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "# New functions for pool-based AL. Different from real-test AL.\n",
    "\n",
    "def recommend_next(train_X,train_Y,test_X, beta):\n",
    "    single_model = SingleTaskGP(train_X,train_Y)\n",
    "    mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    UCB = UpperConfidenceBound(single_model, beta)\n",
    "\n",
    "    ucb = UCB(torch.unsqueeze(test_X,1))\n",
    "    max_idx = np.argmax(ucb.detach().numpy())\n",
    "    max_ucb = max(ucb.detach().numpy())\n",
    "    if max_ucb != ucb.detach().numpy()[max_idx]:\n",
    "        print('wrong index')\n",
    "    return max_idx, test_X[max_idx], test_y[max_idx]\n",
    "\n",
    "def recommend_grid(train_X,train_Y,test_X, beta):\n",
    "    single_model = SingleTaskGP(train_X,train_Y)\n",
    "    mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    UCB = UpperConfidenceBound(single_model, beta)\n",
    "    ucb = UCB(torch.unsqueeze(test_X,1))\n",
    "    max_idx = np.argmax(ucb.detach().numpy())\n",
    "    max_ucb = max(ucb.detach().numpy())\n",
    "    if max_ucb != ucb.detach().numpy()[max_idx]:\n",
    "        print('wrong index')\n",
    "    return max_idx, test_X[max_idx]\n",
    "\n",
    "def acq_section(model,test_X, beta):\n",
    "    UCB = UpperConfidenceBound(model, beta)\n",
    "    ucb = UCB(torch.unsqueeze(test_X,1))\n",
    "    max_idx = np.argmax(ucb.detach().numpy())\n",
    "    max_ucb = max(ucb.detach().numpy())\n",
    "    if max_ucb != ucb.detach().numpy()[max_idx]:\n",
    "        print('wrong index')\n",
    "    return max_idx, test_X[max_idx]\n",
    "    \n",
    "def recommend_Qgrid(train_X,train_Y,test_X, beta):\n",
    "    single_model = SingleTaskGP(train_X,train_Y)\n",
    "    mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    sampler = SobolQMCNormalSampler(1024)\n",
    "    qUCB = qUpperConfidenceBound(single_model, beta, sampler)\n",
    "    qucb = qUCB(torch.unsqueeze(test_X,1))\n",
    "    max_idx = np.argmax(qucb.detach().numpy())\n",
    "    max_ucb = max(qucb.detach().numpy())\n",
    "    if max_ucb != qucb.detach().numpy()[max_idx]:\n",
    "        print('wrong index')\n",
    "    return max_idx, test_X[max_idx]\n",
    "\n",
    "# log results\n",
    "def log_result(test_X,test_Y,max_idx,data_dest):\n",
    "    decode_candidates(test_X)\n",
    "    features = test_X[max_idx].numpy()\n",
    "    obs = test_Y[max_idx].numpy()\n",
    "    df_features = pd.DataFrame(features.reshape(-1, len(features)))\n",
    "    df_obs = pd.DataFrame(obs)\n",
    "    df_log = pd.concat([df_features,df_obs],axis=1)\n",
    "    df_log.columns =['yb_1', 'er_1', 'yb_2', 'er_2','radius','UV']\n",
    "    log = pd.read_csv(data_dest)\n",
    "    log = log.append(df_log, ignore_index=True)\n",
    "    log.to_csv(data_dest, index=False)\n",
    "\n",
    "def append_result(test_X,test_Y,max_idx,log):\n",
    "    decode_candidates(test_X)\n",
    "    features = test_X[max_idx].numpy()\n",
    "    obs = test_Y[max_idx].numpy()\n",
    "    result = np.concatenate((features, obs), axis=None)\n",
    "    log.append(result)\n",
    "    \n",
    "# update pool\n",
    "def update_pool(train_X,train_Y,test_X,test_Y,pool_X, pool_Y, max_idx):\n",
    "    train_X, train_Y = torch.cat([train_X,torch.unsqueeze(pool_X[max_idx],0)]),torch.cat([train_Y,torch.unsqueeze(pool_Y[max_idx],0)])\n",
    "    #print(pool_x[max_idx])\n",
    "    if max_idx ==0:\n",
    "        pool_X, pool_Y = pool_X[1:], pool_y[1:]\n",
    "    elif max_idx ==(len(pool_X)-1):\n",
    "        pool_X, pool_Y = pool_X[:-1], pool_y[:-1]\n",
    "    else:\n",
    "        pool_X, pool_Y = torch.cat([pool_X[:max_idx], pool_X[max_idx+1:]]), torch.cat([pool_Y[0:max_idx], pool_Y[max_idx+1:]]),\n",
    "    test_X, test_Y = torch.clone(pool_X), torch.clone(pool_Y)\n",
    "    return train_X,train_Y,test_X,test_Y,pool_X, pool_Y\n",
    "\n",
    "def add_one_ensemble(log_seed,i):\n",
    "    df = pd.DataFrame((log_seed[i]))\n",
    "    df.columns =['yb_1', 'er_1', 'yb_2', 'er_2','radius','UV']\n",
    "\n",
    "    evalued = len(df)\n",
    "    max_uv = []\n",
    "    y = df.UV\n",
    "    for i in range(evalued):\n",
    "        max_uv.append(df.iloc[0:i+1].UV.max())\n",
    "    x = range(len(max_uv))\n",
    "    max_x = len(max_uv)\n",
    "    max_y = max(max_uv)    \n",
    "    ax1.scatter(x, y, c='black', alpha = 0.3)\n",
    "    ax2.plot(x, max_uv, c='r', alpha = 1)\n",
    "    return max_x, max_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18565c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data log from local: ../saved_data/UV_log_shuffled_10initial_test1_grid_beta=10000.csv!\n"
     ]
    }
   ],
   "source": [
    "# read from cPickle\n",
    "import pickle\n",
    "pool_x = pickle.load( open( \"../saved_data/NP_pool_small_conc2_radi2_26334375NP_encoded.pkl\", \"rb\" ) )\n",
    "train_x, train_y, _ = get_data_botorch(\"../saved_data/UV_log_shuffled_10initial_test1_grid_beta=10000.csv\", from_cloud=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "caee0965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data log from local: ../saved_data/UV_log_shuffled_10initial_test1_grid_beta=10000.csv!\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, _ = get_data_botorch(\"../saved_data/UV_log_shuffled_10initial_test1_grid_beta=10000.csv\", from_cloud=False)\n",
    "encode_inputs(train_x)\n",
    "def recommend_grid_2steps(train_X, train_Y, test_X, pie_size=100000, beta=10000):\n",
    "    # split the original pool to sectors that contains 10,000 grids in each sector\n",
    "    pies = torch.split(test_X, pie_size)\n",
    "    idx_list = []\n",
    "    single_model = SingleTaskGP(train_X,train_Y)\n",
    "    mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
    "    fit_gpytorch_model(mll)\n",
    "\n",
    "    # the candidates from each sector will be saved in 'seleced'\n",
    "    selected = torch.empty((0,5))\n",
    "    for i,pool_i in enumerate(pies):\n",
    "        #print(f'recommending section {i}/{len(pie)} ......')\n",
    "        max_idx, max_feature = acq_section(single_model, pool_i, beta)\n",
    "        selected = torch.cat((selected,torch.unsqueeze(max_feature,0)),0)\n",
    "        idx_list.append(i*pie_size+max_idx)\n",
    "\n",
    "    # the final round of recommedation from the candidataes in the selected pool\n",
    "    max_idx_final, max_feature_final = acq_section(single_model, selected, beta)\n",
    "    return torch.unsqueeze(max_feature_final,0), idx_list[max_idx_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "94b955cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate, idx = recommend_grid_2steps(train_x, train_y, pool_x, pie_size=100000, beta=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "301db491",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_candidates(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "31d30dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9200, 0.9787, 0.0000, 0.5000, 0.9706]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1170f04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26009795"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "74a61da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9200, 0.9787, 0.0000, 0.5000, 0.9706])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1f63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "716d2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_grid(pool_X, max_idx):\n",
    "    if max_idx ==0:\n",
    "        pool_X = pool_X[1:]\n",
    "    elif max_idx ==(len(pool_X)-1):\n",
    "        pool_X = pool_X[:-1]\n",
    "    else:\n",
    "        pool_X = torch.cat([pool_X[:max_idx], pool_X[max_idx+1:]])    \n",
    "    return pool_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e8da8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_x = update_grid(pool_x, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c95a12b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9200, 1.0000, 0.0000, 0.5000, 0.9706])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool_x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pool_x, open( \"../saved_data/NP_pool_small_conc2_radi2_26334375NP_encoded.pkl\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
